{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c254aa-2e83-4ad2-9a8f-a6976054b96a",
   "metadata": {},
   "source": [
    "# Label-free CBM â€” Minimal Evaluation (Simplified)\n",
    "**Auto-generated:** 2025-08-19T23:50:51.252330Z\n",
    "\n",
    "This notebook is simplified to a *single* flow:\n",
    "1) Load bundle\n",
    "2) Force RN50 (1024-d) encoder\n",
    "3) Define `cbm_infer`\n",
    "4) Run inference\n",
    "\n",
    "> Keep using RN50 to match `W_c (281, 1024)`. If you use a 512-d encoder (e.g., ViT-B/16), it will error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93055e7e-cf71-4a0f-bdba-c1bbf84a522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, open_clip\n",
    "\n",
    "# --- Paths ---\n",
    "BUNDLE = \"/kayla/saved_models/lf_cbm_cifar10/lf_cbm_minimal_bundle.pt\"  # edit if needed\n",
    "\n",
    "# --- Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --- Load bundle ---\n",
    "d = torch.load(BUNDLE, map_location=\"cpu\")\n",
    "sd = d[\"state_dict\"]\n",
    "W_c = sd[\"concept_layer.weight\"].to(device)   # [K, 1024]\n",
    "W_g = sd[\"final_layer.weight\"].to(device)     # [C, K]\n",
    "b_g = sd[\"final_layer.bias\"].to(device)       # [C]\n",
    "\n",
    "c_mean = torch.tensor(d.get(\"concept_mean\", []), dtype=torch.float32, device=device) if d.get(\"concept_mean\") is not None else None\n",
    "c_std  = torch.tensor(d.get(\"concept_std\",  []), dtype=torch.float32, device=device) if d.get(\"concept_std\")  is not None else None\n",
    "\n",
    "print(\"W_c:\", tuple(W_c.shape), \"W_g:\", tuple(W_g.shape), \"b_g:\", tuple(b_g.shape))\n",
    "\n",
    "# --- Force RN50 encoder (1024-d) ---\n",
    "_clip_model, _, preprocess = open_clip.create_model_and_transforms(\"RN50\", pretrained=\"openai\")\n",
    "_clip_model = _clip_model.to(device).eval()\n",
    "\n",
    "def ENCODER(imgs_224: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        f = _clip_model.encode_image(imgs_224.to(device))\n",
    "        f = f / (f.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "    return f  # [N, 1024]\n",
    "\n",
    "# Smoke test: should be 1024\n",
    "print(\"Encoder dim:\", ENCODER(torch.zeros(1,3,224,224)).shape[-1])\n",
    "\n",
    "# --- Minimal cbm_infer ---\n",
    "def cbm_infer(x_images_224: torch.Tensor, *, apply_zscore: bool = True, return_probs: bool = True):\n",
    "    with torch.no_grad():\n",
    "        feats = ENCODER(x_images_224)             # [N, 1024]\n",
    "        if feats.shape[-1] != W_c.shape[1]:\n",
    "            raise RuntimeError(f\"Encoder D={feats.shape[-1]} != W_c expects D={W_c.shape[1]}. Use RN50.\")\n",
    "        concepts = feats @ W_c.T                  # [N, K]\n",
    "        if apply_zscore and (c_mean is not None) and (c_std is not None) and (c_mean.numel() == concepts.shape[1]):\n",
    "            concepts = (concepts - c_mean) / (c_std + 1e-6)\n",
    "        logits = concepts @ W_g.T + b_g           # [N, C]\n",
    "        if return_probs:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            return logits, concepts, probs\n",
    "        return logits, concepts\n",
    "\n",
    "print(\"Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c99f0e-d252-4560-be0b-000cf15b4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demo: run on a zero batch (just to prove shapes flow) ---\n",
    "x = torch.zeros(2,3,224,224, device=device)\n",
    "logits, concepts, probs = cbm_infer(x, apply_zscore=True, return_probs=True)\n",
    "print(\"logits:\", logits.shape, \"concepts:\", concepts.shape, \"probs:\", probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae92fe1-1d92-4607-a35f-895b5ee32695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import utils\n",
    "import data_utils\n",
    "import json\n",
    "\n",
    "import cbm\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefee581-6e2f-4da9-92d3-52c0d4889aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the correct model dir, everything else should be taken care of\n",
    "load_dir = \"saved_models/cifar10_cbm_2025_08_19_21_43\"\n",
    "device = \"cuda\"\n",
    "\n",
    "with open(os.path.join(load_dir, \"args.txt\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "dataset = args[\"dataset\"]\n",
    "_, target_preprocess = data_utils.get_target_model(args[\"backbone\"], device)\n",
    "model = cbm.load_cbm(load_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c775075-8636-406f-93fc-133bc3f44ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_d_probe = dataset+\"_val\"\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "\n",
    "val_data_t = data_utils.get_data(val_d_probe, preprocess=target_preprocess)\n",
    "val_pil_data = data_utils.get_data(val_d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cf2db-8814-461d-9a19-fcd23cca7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cls_file, \"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")\n",
    "\n",
    "with open(os.path.join(load_dir, \"concepts.txt\"), \"r\") as f:\n",
    "    concepts = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d588084-4d6a-48c2-b35c-e137a2f90a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d7fdc-caa4-48c7-a8fa-1d71c9c15ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = utils.get_accuracy_cbm(model, val_data_t, device)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec24d7-7592-4bba-ae92-3fb914a5a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show final layer weights for some classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ac842-3175-4028-9ce1-a16cb6819f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can build a Sankey diagram of weights by copying the incoming weights printed below into https://sankeymatic.com/build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61756e0-d76e-4c58-bbfe-1019d225a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = random.choices([i for i in range(len(classes))], k=1)\n",
    "\n",
    "for i in to_show:\n",
    "    print(\"Output class:{} - {}\".format(i, classes[i]))\n",
    "    print(\"Incoming weights:\")\n",
    "    for j in range(len(concepts)):\n",
    "        if torch.abs(model.final.weight[i,j])>0.05:\n",
    "            print(\"{} [{:.4f}] {}\".format(concepts[j], model.final.weight[i,j], classes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7347c-e6d8-4f59-a9bf-80621dcdb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = random.choices([i for i in range(len(classes))], k=2)\n",
    "\n",
    "top_weights, top_weight_ids = torch.topk(model.final.weight, k=5, dim=1)\n",
    "bottom_weights, bottom_weight_ids = torch.topk(model.final.weight, k=5, dim=1, largest=False)\n",
    "\n",
    "for i in to_show:\n",
    "    print(\"Class {} - {}\".format(i, classes[i]))\n",
    "    out = \"Highest weights: \"\n",
    "    for j in range(top_weights.shape[1]):\n",
    "        idx = int(top_weight_ids[i, j].cpu())\n",
    "        out += \"{}:{:.3f}, \".format(concepts[idx], top_weights[i, j])\n",
    "    print(out)\n",
    "    out = \"Lowest weights: \"\n",
    "    for j in range(bottom_weights.shape[1]):\n",
    "        idx = int(bottom_weight_ids[i, j].cpu())\n",
    "        out += \"{}:{:.3f}, \".format(concepts[idx], bottom_weights[i, j])\n",
    "    print(out + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbead4-8f7d-4e94-9015-c39aa04b78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some features may not have any non-zero outgoing weights, \n",
    "# i.e. these are not used by the model and should be deleted for better performance\n",
    "weight_contribs = torch.sum(torch.abs(model.final.weight), dim=0)\n",
    "print(\"Num concepts with outgoing weights:{}/{}\".format(torch.sum(weight_contribs>1e-5), len(weight_contribs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fdd8f-954c-4c5a-8ddd-3381ab360571",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explain model reasoning for random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345f3b7-4008-4e3a-9233-db9b5f2ef414",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_display = random.sample([i for i in range(len(val_pil_data))], k=4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in to_display:\n",
    "        image, label = val_pil_data[i]\n",
    "        x, _ = val_data_t[i]\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        display(image.resize([320,320]))\n",
    "        \n",
    "        outputs, concept_act = model(x)\n",
    "        \n",
    "        top_logit_vals, top_classes = torch.topk(outputs[0], dim=0, k=2)\n",
    "        conf = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        print(\"Image:{} Gt:{}, 1st Pred:{}, {:.3f}, 2nd Pred:{}, {:.3f}\".format(i, classes[int(label)], classes[top_classes[0]], top_logit_vals[0],\n",
    "                                                                      classes[top_classes[1]], top_logit_vals[1]))\n",
    "        \n",
    "        for k in range(1):\n",
    "            contributions = concept_act[0]*model.final.weight[top_classes[k], :]\n",
    "            feature_names = [(\"NOT \" if concept_act[0][i] < 0 else \"\") + concepts[i] for i in range(len(concepts))]\n",
    "            values = contributions.cpu().numpy()\n",
    "            max_display = min(int(sum(abs(values)>0.005))+1, 8)\n",
    "            title = \"Pred:{} - Conf: {:.3f} - Logit:{:.2f} - Bias:{:.2f}\".format(classes[top_classes[k]],\n",
    "                             conf[top_classes[k]], top_logit_vals[k], model.final.bias[top_classes[k]])\n",
    "            plots.bar(values, feature_names, max_display=max_display, title=title, fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
